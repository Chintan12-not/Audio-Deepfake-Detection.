Hereâ€™s your **final README** with the updated **Recall** and **F1 Score** metrics:  

---

# ğŸ™ï¸ Audio Deepfake Detection - Momenta Take-Home Assessment  

## ğŸ“Œ Overview  
This project focuses on detecting **AI-generated speech (audio deepfakes)** using machine learning models. The goal is to identify manipulated voice recordings using **deep learning techniques** and evaluate their effectiveness.  

---

## ğŸ“Š Selected Models  
We analyzed multiple deepfake detection approaches and selected the following:  

### 1ï¸âƒ£ **VALL-E (Microsoft)**  
ğŸ”¹ **Key Innovation:** Generates speech based on text input with **zero-shot learning**.  
ğŸ”¹ **Performance:** High speech synthesis quality but lacks real-time detection capabilities.  
ğŸ”¹ **Why Chosen:** Advanced deepfake generation helps understand detection challenges.  
ğŸ”¹ **Limitations:** Requires large datasets and high computational power.  

### 2ï¸âƒ£ **Pengi (Microsoft)**  
ğŸ”¹ **Key Innovation:** Uses **pretrained embeddings** for audio analysis.  
ğŸ”¹ **Performance:** Effective for speech synthesis detection.  
ğŸ”¹ **Why Chosen:** Strong feature extraction capabilities.  
ğŸ”¹ **Limitations:** Lacks robustness for unseen deepfake models.  

### 3ï¸âƒ£ **AudioPaLM (Google)**  
ğŸ”¹ **Key Innovation:** Combines **audio processing with large language models**.  
ğŸ”¹ **Performance:** Strong at capturing phonetic variations.  
ğŸ”¹ **Why Chosen:** Hybrid approach improves detection.  
ğŸ”¹ **Limitations:** Requires specialized hardware for training.  

---

## ğŸ“‚ Dataset  
We used the **ASVspoof 5 dataset**, a widely recognized dataset for detecting spoofed audio samples.  

---

## ğŸµ Audio Preprocessing  
To prepare audio files for deepfake detection, we initially used the following steps:  

1ï¸âƒ£ **FLAC to WAV Conversion**  
- Since some datasets contain `.flac` files, they were converted to `.wav` format for compatibility with our model.  
- This was done using **FFmpeg** or Pythonâ€™s `pydub` library.  

2ï¸âƒ£ **Feature Extraction - MFCC**  
- **Mel-Frequency Cepstral Coefficients (MFCCs)** were extracted to represent audio signals in a way that captures human speech characteristics.  
- This helped improve model accuracy by providing meaningful representations of voice patterns.  

âš ï¸ **Note:** These steps are not included in the final code but were part of our initial pipeline. Users may add them if working with raw `.flac` files.  

---

## ğŸ”¥ Implementation  
We implemented a **CNN-LSTM model** for deepfake detection.  

### **Model Architecture:**  
âœ… **Conv1D + MaxPooling1D** - Extracts time-series audio features.  
âœ… **Flatten Layer** - Converts extracted features into a format usable by LSTMs.  
âœ… **LSTM Layers** - Captures sequential dependencies in the audio data.  
âœ… **Dropout & Dense Layer** - Prevents overfitting & performs classification.  

---

## ğŸ† Fine-Tuning & Evaluation  
We fine-tuned our model using **transfer learning** techniques and evaluated it using:  

ğŸ”¹ **Accuracy**  
ğŸ”¹ **Precision**  
ğŸ”¹ **Recall**  
ğŸ”¹ **F1 Score**  

### **Results:**  
| Model         | Accuracy | Precision | Recall | F1 Score |  
|--------------|----------|-----------|--------|----------|  
| **CNN**      | **0.8350** | **0.1739** | **0.1524** | **0.1624** |  
| **LSTM**     | **0.8950** | **0.0000** | **0.0000** | **0.0000** |  
| **CNN-LSTM** | **0.8950** | **0.0000** | **0.0000** | **0.0000** |  

---

## ğŸ“Œ Key Challenges & Solutions  
âœ… **Handling Imbalanced Data:** Used oversampling techniques.  
âœ… **Computational Limitations:** Optimized hyperparameters to reduce training time.  
âœ… **Feature Extraction Issues:** Applied spectral analysis for better representation.  

---

## ğŸš€ Future Improvements  
ğŸ”¹ **Enhancing Real-Time Detection:** Optimizing for faster inference.  
ğŸ”¹ **Multi-Language Support:** Expanding dataset diversity.  
ğŸ”¹ **Integration with Voice Authentication Systems.**  

---

## ğŸ› ï¸ Setup & Installation  
To run this project locally, follow these steps:  

1ï¸âƒ£ **Clone the Repository:**  
```sh
git clone https://github.com/Chintan12-not/Audio-Deepfake-Detection.git
```

2ï¸âƒ£ **Navigate to the Project Directory:**  
```sh
cd Audio-Deepfake-Detection
```

3ï¸âƒ£ **Install Required Dependencies:**  
```sh
pip install -r requirements.txt
```

4ï¸âƒ£ **Run the Model:**  
```sh
python train_model.py
```

5ï¸âƒ£ **Evaluate the Model:**  
```sh
python evaluate.py
```

---

## ğŸ“ Conclusion  
This project successfully applies **deep learning techniques** to detect **AI-generated speech (deepfakes)**. While the **CNN-LSTM model** shows promising results, future improvements can enhance real-time detection, generalization, and accuracy across different datasets.  
